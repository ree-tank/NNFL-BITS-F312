{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L_layered_NN.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPwYVBBoBK64d5QSKyXw337"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"mj9W2op5t48H"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VUcKHQtxuEkc"},"source":["def split(X, Y, val_split = 0.1, test_split = 0.2):\n","  '''\n","  splits the input and output in\n","  train, validation and test set\n","  '''\n","  train_size = int(X.shape[1] * (1 - (val_split + test_split)))\n","  val_size = int(X.shape[1] * val_split)\n","  test_size = X.shape[1] - train_size - val_size                     \n","\n","  X_tr = X.iloc[:, :train_size]\n","  Y_tr = Y.iloc[:, :train_size]\n","\n","  X_val = X.iloc[:, train_size:train_size + val_size]\n","  Y_val = Y.iloc[:, train_size:train_size + val_size]\n","  X_val.columns = list(range(val_size))                       ### to set column name from 0 to val_size\n","  Y_val.columns = list(range(val_size))\n","\n","  X_ts = X.iloc[:, train_size + val_size:]\n","  Y_ts = Y.iloc[:, train_size + val_size:]\n","  X_ts.columns = list(range(test_size))                       ### to set column name from 0 to test_size\n","  Y_ts.columns = list(range(test_size))\n","\n","  return X_tr, Y_tr, X_val, Y_val, X_ts, Y_ts"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GD_wauM5uLB_"},"source":["def initialize_params(dims, seed):\n","  '''\n","  dims: number of neurons in each layer\n","  returns:\n","  parameters: python dict containing W1, b1, w2, b2...., WL, bL\n","              W1--- weight matrix of shape(dims[l], dims[l-1])\n","              b1--- bias matrix of shape(dims[l], 1)\n","\n","  '''\n","  np.random.seed(seed)\n","  parameters = {}  ## empty dict\n","  L = len(dims)    ## number of layers in NN\n","\n","  for l in range(1, L):\n","    parameters[\"W\" + str(l)] = np.random.randn(dims[l], dims[l-1])\n","    parameters[\"b\" + str(l)] = np.zeros((dims[l], 1))\n","\n","  return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UqSIUrPHdITc"},"source":["def linear_forward(A, W, b):\n","  '''\n","  A: activations from previous layer\n","  W: weight matrix of shape(size of surrent layer, size of prev layer)\n","  b: bias matrix of shape(size_current_layer, 1)\n","\n","  returns:\n","  Z: W*A + b --- the input of activation function\n","  cache: (A, W, b) --- for backpropagation\n","  '''\n","  Z = np.dot(W,A) + b\n","  cache = (A, W, b)\n","\n","  return Z, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J9jt8ShOgzHe"},"source":["def sigmoid(Z):\n","  '''\n","  Z: array of any shape\n","  \n","  returns:\n","  A = sigmoid(Z)\n","  cache = Z ---- helpful in backprop\n","  '''\n","  A = 1/(1 + np.exp(-Z))\n","  cache = Z\n","  return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rINToW09hZ_u"},"source":["def relu(Z):\n","  '''\n","  Z: array of any shape\n","\n","  returns:\n","  A = max(0, Z)\n","  cache = Z --- helpful in backprop\n","  '''\n","  A = np.maximum(0, Z)\n","  cache = Z\n","  return A, cache"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w5fYJjjrui6L"},"source":["def linear_activation_forward(A_prev, W, b, activation):\n","  '''\n","  A_prev: Activations from prev layer\n","  W: weight matrix\n","  b: bias matrix\n","  activation: either relu or sigmoid\n","\n","  returns:\n","  A: activation values\n","  cache: (linear_cache, activation_cache)\n","  '''\n","  if activation == \"sigmoid\":\n","    Z, linear_cache = linear_forward(A_prev, W, b)     ### Z stores = A_prev * W + b, linear_cache stores = (A_prev, W, b)\n","    A, activation_cache = sigmoid(Z)                   ### A stores = sigmoid(Z),    activation_cache stores = (Z)\n","\n","  elif activation == \"relu\":\n","    Z, linear_cache = linear_forward(A_prev, W, b)    ### Z stores = A_prev * W + b, linear_cache stores = (A_prev, W, b)\n","    A, activation_cache = relu(Z)                     ### ### A stores = sigmoid(Z),    activation_cache stores = (Z)\n","\n","  cache = (linear_cache, activation_cache)            ### cache stores = ((A_prev, W, b), (Z))\n","  return A, cache                                     ### this function returns A, ((A_prev, W, b), (Z)) ---- helpful in backprop"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vw9IHpZ0upbI"},"source":["def l_layer_forward(X, parameters):\n","  '''\n","  for l layered NN, this function will calculate relu activation for (l-1) layers and sigmoid activatio for last layer\n","  X: input\n","  parameters: initialized parameters\n","\n","  returns:\n","  AL: last layer activation value\n","  caches: list of caches containing:\n","          linear_relu_cache (there will be l-1 of these caches ---- indexed from 0 to l-2)\n","          linear_sigmoid_cache (there will 1 cache  ------ indexed l-1)\n","  '''\n","  caches = []\n","  L = len(parameters)//2             ### number of layers in NN\n","  A = X                              ### Activation of input layer is X\n","\n","  for l in range(1, L):             ### for l-1 relu layers\n","    A_prev = A\n","    A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], \"relu\")\n","    caches.append(cache)       ### caches stores (A_prev, W, b, Z) for every layer\n","\n","  AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")  ### for sigmoid layer\n","  caches.append(cache)\n","\n","  return AL, caches"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pkASTVykuuD0"},"source":["def compute_cost(AL, Y):\n","  '''\n","  AL: activations of last layer of shape (1, number of examples)\n","  Y: actual output labels of shape (1, number of examples)\n","\n","  returns:\n","  cost: cross_entropy loss\n","  '''\n","  m = Y.shape[1]\n","  epsilon = 1e-5\n","  cost_val = np.multiply(Y, np.log(AL+epsilon)) + np.multiply((1-Y), np.log(1-AL+epsilon))\n","  cost = -1/m * cost_val\n","\n","  cost = np.squeeze(cost)         ### convert [[1.23]] in 1.23\n","  return cost"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xhSHhTfMux-O"},"source":["def linear_backward(dZ, cache):\n","  '''\n","  dZ: gradient of cost wrt Z of current layer (l)\n","  cache: (A_prev, W, b)\n","  \n","  returns:\n","  dW: gradient of cost wrt W (current_layer) ---- same shape as W\n","  db: gradient of bias wrt b (current layer) ---- same shape as b\n","  dA_prev: gradient of cost wrt to activation of previous layer ---- same shape as A_prev\n","  '''\n","  A_prev, W, b = cache\n","  m = A_prev.shape[1]\n","\n","  dW = 1/m * np.dot(dZ, A_prev.T)\n","  db = 1/m * np.sum(dZ,axis=1,keepdims = True)\n","  dA_prev = np.dot(W.T, dZ)\n","\n","  return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nwjePR1Xu1px"},"source":["def sigmoid_backward(dA, cache):\n","  '''\n","  dA: gradient of activation wrt cost ---- dA = dCost/dA\n","  cache: 'Z' that we stored earlier\n","\n","  returns\n","  dZ: gradient of cost wrt to Z ----- dZ = dCost/dZ = dCost/dA * dA/dZ\n","\n","  for sigmoid: dA/dZ = A*(1-A)\n","  '''\n","  Z = cache\n","  epsilon = 1e-3\n","  s = 1/(1+np.exp(-Z)+epsilon)\n","  dZ = dA * s * (1-s)\n","  return dZ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IyUaSXbZu47H"},"source":["def relu_backward(dA, cache):\n","  '''\n","  dA: gradient of activation wrt cost ---- dA = dCost/dA\n","  cache: 'Z' that we stored earlier\n","\n","  returns\n","  dZ: gradient of cost wrt to Z ----- dZ = dCost/dZ = dCost/dA * dA/dZ\n","  '''\n","  Z = cache\n","  dZ = np.array(dA, copy=True)\n","  dZ[Z<=0] = 0\n","  return dZ"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y9TpqKh5u8dE"},"source":["def linear_activation_backward(dA, cache, activation):\n","  '''\n","  dA: gradient of activation wrt cost\n","  cache: (linear_cache, activation_cache)\n","  activation: either sigmoid or relu\n","\n","  returns:\n","  dA_prev: gradient of cost wrt A_prev (layer l-1)\n","  dW: gradient of cost wrt W --- current layer\n","  db: gradient of cost wrt b --- current layer\n","  ''' \n","  linear_cache, activation_cache = cache     #### linear_cache stores: (A_prev, W, b) and activation_cache stores: (Z)\n","  if activation == \"sigmoid\":\n","    dZ = sigmoid_backward(dA, activation_cache)\n","\n","  elif activation == \"relu\":\n","    dZ = relu_backward(dA, activation_cache)\n","\n","  dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","  return dA_prev, dW, db"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"378ydcTFu_ns"},"source":["def l_layer_backward(AL, Y, caches):\n","  '''\n","  to initialize backprop, we need to calculate dAL ---- dAL = dCost/dAL\n","\n","  AL: activation of last layer\n","  Y: actual output labels\n","  caches: list of caches containing:\n","          every linear_activation_cache with relu function --- indexed from 0 to l-2(because we have l-1 relu layers)\n","          linear_activation_cache with sigmoid function ---- indexed at L-1\n","\n","  returns: \n","  grads dict containing\n","  dA, dW, db\n","  '''\n","  grads = {}\n","  L = len(caches)\n","  m = Y.shape[1]\n","  Y = Y.reshape(AL.shape)\n","  epsilon = 1e-3\n","  dAL = -(np.divide(Y, AL+epsilon) - np.divide((1-Y), (1-AL+epsilon)))\n","  current_cache = caches[L-1]\n","  grads[\"dA\"+str(L-1)], grads[\"dW\"+str(L)], grads[\"db\"+str(L)] = linear_activation_backward(dAL, current_cache, \"sigmoid\")\n","\n","  for l in reversed(range(L-1)):\n","    current_cache = caches[l]\n","    dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, \"relu\")\n","    grads[\"dA\"+str(l)] = dA_prev_temp\n","    grads[\"dW\"+str(l+1)] = dW_temp\n","    grads[\"db\"+str(l+1)] = db_temp\n","\n","  return grads"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U56trPSHvDZw"},"source":["def update_params(parameters, grads, learning_rate):\n","  '''\n","  parameters: dict containing parameters\n","  grads: dict containing gradients\n","  learning_rate: learning_rate of model\n","\n","  returns:\n","  parameters: dict containing updated parameters\n","  '''\n","  L = len(parameters)//2\n","  for l in range(L):\n","    parameters[\"W\"+str(l+1)] = parameters[\"W\"+str(l+1)] - learning_rate * grads[\"dW\"+str(l+1)]\n","    parameters[\"b\"+str(l+1)] = parameters[\"b\"+str(l+1)] - learning_rate * grads[\"db\"+str(l+1)]\n","\n","  return parameters"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJh-rVG50D1p","executionInfo":{"status":"ok","timestamp":1605593232357,"user_tz":-330,"elapsed":1317,"user":{"displayName":"REETANK RASTOGI","photoUrl":"","userId":"16710774968605988725"}}},"source":["def predict(X, Y, parameters):\n","  '''\n","  X: data set that we want to predict labels of \n","  Y: actual labels of data set\n","  parameters: final parameters obtained after training\n","\n","  returns:\n","  predicted labels\n","  '''\n","  m = X.shape[1]\n","  L = len(parameters)//2\n","  Y_pred = np.zeros((1, m))\n","\n","  A_L, caches = l_layer_forward(X, parameters)\n","  #print(A_L)\n","  for i in range(m):\n","    if A_L[0,i] >= 0.5:\n","      Y_pred[0,i] = 1\n","    else:\n","      Y_pred[0,i] = 0\n","\n","  return pd.DataFrame(Y_pred)"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"NH1lz5BGvHQ6"},"source":["def confusion_matrix(Y, Y_pred):\n","  '''\n","  Y: actual labels\n","  Y_pred: predicted labels\n","\n","  returns\n","  confusion_matrix\n","  '''\n","  df_compare = Y.copy()\n","  df_compare[1] = Y_pred\n","  df_compare.columns = ['Actual', 'Predicted']\n","  conf_mat = pd.crosstab(df_compare['Actual'], df_compare['Predicted'], rownames = ['Actual'], colnames = ['Predicted'])\n","  return conf_mat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RW0q6OoThBA9"},"source":["def performance_measure(confusion_matrix):\n","  '''\n","  returns accuracy, specificity and sensitivity\n","  '''\n","  accuracy = (confusion_matrix[0][0] + confusion_matrix[1][1])/(np.sum(np.sum(confusion_matrix)))\n","  specificity = (confusion_matrix[0][0])/(np.sum(confusion_matrix[0]))\n","  sensitivity = (confusion_matrix[1][1])/(np.sum(confusion_matrix[1]))\n","  return accuracy, specificity, sensitivity"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sQ_ZD1PmhE_N"},"source":["def normalize(X, min_features, max_features):\n","  n = X.shape[0]\n","  m = X.shape[1]\n","  X1 = pd.DataFrame(np.zeros((n,m)))\n","  for i in range(n):\n","    X1.iloc[i,:] = (X.iloc[i,:] - min_features[i])/(max_features[i] - min_features[i])\n","  return X1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Q5DMkNfiG5X"},"source":[""],"execution_count":null,"outputs":[]}]}